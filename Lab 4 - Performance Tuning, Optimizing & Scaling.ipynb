{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "af19f4f2-9fb3-4c12-89ab-e6223df6828c",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# 🚀 **Lab 4 - Performance Tuning, Optimization & Scaling**\n",
                "In this lab, you'll explore key concepts and techniques for tuning performance, selecting optimal Delta configs, and other critical concepts to maintain a scalable lakehouse architecture. Let's dive in!  \n",
                "\n",
                "## 🎯 What You'll Learn\n",
                "- The Native Execution Engine\n",
                "- Optimizing tables with zone and use case appropriate table features\n",
                "- Table clustering\n",
                "- Compute sizing\n",
                "- Query optimization\n",
                "- Table maintenance"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "355afaf6-72c1-4f39-8525-39c16bdcc9f7",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## **4.1 - Why Spark & Delta Performance Tuning Matters** | 🕑 5 min.\n",
                "#### 🧠 Why Performance Tuning?\n",
                "Performance tuning is not just about speed—it’s about:\n",
                "- 💰 **Reducing cost** (fewer resources, faster jobs)\n",
                "- 📈 **Improving scalability** (handle more data, more users)\n",
                "- 📊 **Delivering consistent SLAs** (avoid those surprise slowdowns)\n",
                "- 🧘 **Creating a smoother dev & user experience** (debug less, wait less)\n",
                "\n",
                "In Spark + Delta workloads, **every layer matters**—from the execution engine and data layout to the shape of your compute and how queries are written. Getting performance right means **pulling the right levers at the right time**\n",
                "\n",
                "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-4-tuning-optimizing-scaling/_media/overview.excalidraw.png?raw=true)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e6173d74-41ae-452a-a143-89a56c20df5d",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## **4.2 - Going Native for Big Gains** | 🕑 15 min.\n",
                "### **4.2.0 Why the Native Execution Engine?**\n",
                "#### **4.2.0.1 Columnar vs. Row Memory**\n",
                "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-4-tuning-optimizing-scaling/_media/nee-vs-spark-jvm.excalidraw.png?raw=true)\n",
                "\n",
                "<br>\n",
                "\n",
                "#### **4.2.0.2 Vectorized vs. Scalar Hardware Processing**\n",
                "![](https://github.com/voidfunction/FabCon25SparkWorkshop/blob/main/module-4-tuning-optimizing-scaling/_media/nee-vs-spark-jvm-simd.excalidraw.png?raw=true)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8932a0c6-cec7-4a60-ad6d-b8b541b4661c",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 💪 **4.2.1 - The Power of the Native Execution Engine**"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "97e2db28-901e-4127-9d90-869d72a5b65b",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### **4.2.1.1 - Create benchmark function**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "e4ab211d-5950-40d0-b26a-b8855628159b",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-25T04:21:49.511695Z",
                            "execution_start_time": "2025-03-25T04:21:49.0906986Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "fafb60f5-42f9-402f-8f51-85c1c25645db",
                            "queued_time": "2025-03-25T04:21:39.2941873Z",
                            "session_id": "59d6dd39-09c7-4fd8-a0d2-3306f18d4fbf",
                            "session_start_time": "2025-03-25T04:21:39.2952243Z",
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 3,
                            "statement_ids": [
                                3
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, 59d6dd39-09c7-4fd8-a0d2-3306f18d4fbf, 3, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "def benchmark_native_engine(query):\n",
                "    import time\n",
                "    spark.conf.set(\"spark.synapse.vegas.useCache\", \"false\")\n",
                "    spark.conf.set(\"spark.native.enabled\", \"false\")\n",
                "    # Set description of the job\n",
                "    spark.sparkContext.setJobDescription(f\"Spark Query: {query}\")\n",
                "    # Record start time\n",
                "    start_time_spark = time.time()\n",
                "    # Execute query\n",
                "    q = spark.sql(query).limit(1000)\n",
                "    # Collect results (this triggers the query execution)\n",
                "    if query.strip().replace('\\n', '').split(' ')[0].lower() in ('with', 'select'):\n",
                "        q.collect()\n",
                "    # Record end time\n",
                "    end_time_spark = time.time()\n",
                "    duration_spark = (end_time_spark - start_time_spark) * 1000  # Convert to milliseconds\n",
                "    print(f\"Execution time w/ Spark: {duration_spark:.2f} ms\")\n",
                "\n",
                "    spark.conf.set(\"spark.native.enabled\", \"true\")\n",
                "    # Set description of the job\n",
                "    spark.sparkContext.setJobDescription(f\"Native Query: {query}\")\n",
                "    # Record start time\n",
                "    start_time_native = time.time()\n",
                "    # Execute query\n",
                "    q = spark.sql(query).limit(1000)\n",
                "    # Collect results (this triggers the query execution)\n",
                "    if query.strip().replace('\\n', '').split(' ')[0].lower() in ('with', 'select'):\n",
                "        q.collect()\n",
                "    # Record end time\n",
                "    end_time_native = time.time()\n",
                "    spark.sparkContext.setJobDescription(None)\n",
                "    duration_native = (end_time_native - start_time_native) * 1000  # Convert to milliseconds\n",
                "\n",
                "    # Get the execution plan\n",
                "    execution_plan = q._jdf.queryExecution().executedPlan().toString()\n",
                "\n",
                "    # Assert that the plan contains \"Velox\" to make sure that NEE was enabled and had at least 1 compatible operation\n",
                "    assert \"Velox\" in execution_plan, f\"Plan did not contain Velox: {execution_plan}\"\n",
                "\n",
                "    spark.conf.set(\"spark.synapse.vegas.useCache\", \"true\")\n",
                "\n",
                "    print(f\"Execution time w/ Native: {duration_native:.2f} ms\")\n",
                "    times_faster = duration_spark/duration_native\n",
                "    if duration_spark > duration_native:\n",
                "        print(f\"Native was \\033[1;34m{times_faster:.1f}x faster\\033[0m!!!\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "33e51e6e-68df-43ef-8b93-b499773027ea",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### **4.2.1.2 - Run Benchmark on 1.5B row parquet dataset**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "601bc23f-f07f-4672-aeb7-d81999bf8cbc",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-25T04:37:24.6120978Z",
                            "execution_start_time": "2025-03-25T04:37:20.8061885Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "34cecd6d-54f2-48a4-bd83-108749f6b8d7",
                            "queued_time": "2025-03-25T04:37:12.6104439Z",
                            "session_id": "d3796a3c-3e9a-4d18-9ab2-6ff109678345",
                            "session_start_time": "2025-03-25T04:37:12.6114636Z",
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 3,
                            "statement_ids": [
                                3
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, d3796a3c-3e9a-4d18-9ab2-6ff109678345, 3, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "taxi_df = spark.read.parquet(f\"abfss://47938747-73b4-4f78-99dc-4ff2afa78142@onelake.dfs.fabric.microsoft.com/443d992d-01f1-4caa-9345-088e81dd81df/Files/nyctlc/yellow\")\n",
                "taxi_df.createOrReplaceTempView('nyc_yellow_taxi')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "009beecb-1623-44ae-bd2d-20255c99b7e6",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "benchmark_native_engine(\"\"\"\n",
                "    SELECT \n",
                "        AVG(tripDistance), \n",
                "        VendorID \n",
                "    FROM nyc_yellow_taxi\n",
                "    GROUP BY ALL\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a427d92e-b147-4eb6-a2b0-030246ca66c1",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "📌 **Challenge:** write your own query and execute it with the benchmark function: `benchmark_native_engine()`. Reference the DataFrame via the temp view `nyc_yellow_taxi` just like it is any other Lakehouse table or view. \n",
                "\n",
                "💡 **Tip:** you can call `.printSchema()` on any DataFrame (i.e. `taxi_df`) to see what columns are available and the respective data types."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7715a262-2445-479e-9fa9-6fb9189ba05b",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a6c9aaf-b16a-4b13-bb14-6cb37b5619a9",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "benchmark_native_engine(\"\"\"\n",
                "    SELECT \n",
                "\n",
                "    FROM nyc_yellow_taxi\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6437f271-8400-4fd3-87d1-b85b204b1930",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "<details>\n",
                "  <summary><strong>🔑 Answer:</strong>  Click to expand the answer if you need help with this task</summary>\n",
                "\n",
                "~~~python\n",
                "# Print the schema\n",
                "taxi_df.printSchema()\n",
                "\n",
                "# Write a SELECT statement\n",
                "benchmark_native_engine(\"\"\"\n",
                "    SELECT *\n",
                "    FROM nyc_yellow_taxi\n",
                "    LIMIT 1000\n",
                "\"\"\")\n",
                "~~~\n",
                "  \n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d6c4cca1-aee3-42de-9ce7-3cb96edc7fdc",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### **4.2.1.3 - Compare Final Execution Plans**\n",
                "- **Numbering (`*(1)`, `*(2)`) Represents Execution Order**\n",
                "- **Caret symbol (`^`) Represents a Stage Executed via the Velox engine (NEE)**\n",
                "- Graphical plans show Native execution in <span style=\"color:green\">Green</span> and Spark in <span style=\"color:blue\"> blue</span>\n",
                "\n",
                "📌 **Challenge:** open the URL in a new tab and go to the _SQL / DataFrame_ tab. Then compare the difference between SQL queries that start with `Spark Query:` vs. `Native Query` (this job description is coded into the benchmark_native_engine() function).\n",
                "\n",
                "💡 **Tip:** You can call `spark.sparkContext.uiWebUrl` to get the link to the Spark UI. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "41e8d2d0-7286-4dab-a8a9-a6588acacd22",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "spark.sparkContext.uiWebUrl"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "95274a59-93a7-4c69-8c9e-fd414946b055",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### **4.2.2 - Enable the Native Execution Engine**\n",
                "We can enable any mutable Spark configuration via PySpark, Scala, or SparkSQL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "b31229a2-d60c-4697-88e4-ed6588ea1f8d",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-25T04:39:41.8984106Z",
                            "execution_start_time": "2025-03-25T04:39:39.4341166Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "52f72a0a-5fd0-4b2c-9bd0-58469e17bd08",
                            "queued_time": "2025-03-25T04:39:30.070863Z",
                            "session_id": "259855e6-c78d-4dfb-9014-6f56c59cedfe",
                            "session_start_time": "2025-03-25T04:39:30.0718163Z",
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 3,
                            "statement_ids": [
                                3
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, 259855e6-c78d-4dfb-9014-6f56c59cedfe, 3, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "DataFrame[key: string, value: string]"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# PySpark / Scala\n",
                "spark.conf.set(\"spark.native.enabled\", \"true\")\n",
                "\n",
                "# SparkSQL\n",
                "spark.sql(\"SET spark.native.enabled = True\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "472a6399-754c-4340-9a29-c587f1603c10",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### **4.2.2.1 - Audit Session Configs**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9e9bb53e-964a-459f-957b-bbe8a0387ecb",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "# Verify that Spark config is set\n",
                "spark.conf.get(\"spark.native.enabled\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0011c9d9-b3e7-4392-a09b-90e2772351ce",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "##### 🚨 Key Takeaway: **Not Using the Native Execution Engine is a Missed Opportunity**\n",
                "\n",
                "- If you aren't using the Native Execution, and thus operating on the traditional Spark JVM-based execution engine, you’re **missing out on huge performance gains**—we’re talking 2x–5x improvements in many cases.\n",
                "- ✅ *Fix*: Validate engine usage via the Spark UI or logs. Ensure compatible functions and workloads are used to **unlock native execution**."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3ad3819-3f44-4388-82ca-cad4aef9144f",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## **4.3 - Smart Feature Selection by Zone & Workload** | 🕑 10 min.\n",
                "While default Spark configurations should benefit the most common use cases, sometimes we need to modify configurations to optimize our specific workloads. Here we will set spark configurations to optimize typical bronze and silver zones."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "858c862a-ef92-4f70-b19d-a15aea4669f4",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "![OptimizeWrite](https://milescole.dev/assets/img/posts/Optimized-Writes/optimized-write.excalidraw.png)\n",
                "\n",
                "![Deletion Vectors](https://milescole.dev/assets/img/posts/Deletion-Vectors/deletion-vectors-tldr2.excalidraw.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "415f6bbf-f292-4fa3-a0a4-dbcc184e4a25",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### **4.3.1.1 - Get Baseline Perf Before Config Modification**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4846ad8c-a105-48ec-bfb9-f7674bca6b43",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "UPDATE gold.dbo.patientobservations SET deceasedDateTime = NULL WHERE first_name = 'Dion244' and last_name = 'Bergnaum523'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c5c12642-2010-45b2-bec0-7f489b63c15f",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### **4.3.2 - Enable Session Configs**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "d77ece3b-94c6-4b84-9ed8-b5a281a3b8a9",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-24T21:22:14.9832654Z",
                            "execution_start_time": "2025-03-24T21:22:14.6766955Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "ec4339b5-28e7-4a16-a62e-a670d964cf14",
                            "queued_time": "2025-03-24T21:22:14.6755254Z",
                            "session_id": "765ae0fd-357a-4234-bf0a-0c317eeca3f7",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 18,
                            "statement_ids": [
                                18
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, 765ae0fd-357a-4234-bf0a-0c317eeca3f7, 18, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.partitioned.enabled\", \"true\") \n",
                "    # optimizeWrite is generally beneficial for partitioned tables\n",
                "spark.conf.unset(\"spark.databricks.delta.optimizeWrite.enabled\") \n",
                "    # unset so that `optimizeWrite.partitioned.enabled` is not overridden\n",
                "spark.conf.set(\"spark.databricks.delta.optimizeWrite.binSize\", \"128m\") \n",
                "    # appropriate target file size for small tables up to 10GB in size\n",
                "spark.conf.unset(\"spark.sql.parquet.vorder.default\") \n",
                "    # unset so that vorder is not enabled by default at the session level and can be controlled at the table level as needed\n",
                "spark.conf.set(\"spark.databricks.delta.properties.defaults.enableDeletionVectors\", \"true\") \n",
                "    # improve performance of performing updates and deletes\n",
                "spark.conf.set(\"spark.databricks.delta.optimize.maxFileSize\", \"128m\") \n",
                "    # appropriate target file size for small tables up to 10GB in size"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "acde82d5-3878-496d-9da5-7d9c01df2c8e",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### **4.3.2.1 - Enable Delta Features on an Existing Table**\n",
                "Now that we've tailored our Spark configurations to our specific workload. Newly created Delta tables will inherit these configurations. We can modify the V-Order and Deletion Vector properties on existing tables by altering the tables:"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "39d7f1f5-4566-4080-a5c7-e3534557781f",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "39791005-3666-4006-b90a-fca2d0cfa399",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-24T21:22:20.8993202Z",
                            "execution_start_time": "2025-03-24T21:22:19.4668894Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "be9fc573-ac85-4dce-8518-8ccacb64a901",
                            "queued_time": "2025-03-24T21:22:19.4656116Z",
                            "session_id": "765ae0fd-357a-4234-bf0a-0c317eeca3f7",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 19,
                            "statement_ids": [
                                19
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, 765ae0fd-357a-4234-bf0a-0c317eeca3f7, 19, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.synapse.sparksql-result+json": {
                            "data": [],
                            "schema": {
                                "fields": [],
                                "type": "struct"
                            }
                        },
                        "text/plain": [
                            "<Spark SQL result set with 0 rows and 0 fields>"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%sql\n",
                "ALTER TABLE gold.dbo.patientobservations\n",
                "SET TBLPROPERTIES (\n",
                "    'delta.parquet.vorder.enabled' = 'false',\n",
                "    'delta.enableDeletionVectors' = 'true'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb49265e-e4ea-4bd3-8da7-f468809039e1",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### **4.3.2.2 - Audit Delta Table Features**\n",
                "To verify that our table has the desired optimal features, we can use `DESCRIBE DETAIL` using SparkSQL to check what features are enabled on the table. \n",
                "\n",
                "📌 **Challenge:** Use `DESCRIBE DETAIL` to verify  that Deletion Vectors are enabled and that V-Order is not specified or disabled on `gold.dbo.patientobservations`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fb4c6ee1-5925-402f-bfb8-cfe3cf438e26",
            "metadata": {
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f83306bc-8982-4760-8a32-9af434bbb2df",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "<details>\n",
                "  <summary><strong>🔑 Answer:</strong>  Click to expand the answer if you need help with this task</summary>\n",
                "\n",
                "~~~sql\n",
                "DESCRIBE DETAIL golddenormalized.dbo.patientobservations_gold\n",
                "~~~\n",
                "  \n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7a8df28d-c389-425e-8798-f1b159bc2e96",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### **4.3.2.3 - Run Statement with Write Optimized Delta Features**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1312fb1-47a9-47e8-b456-666a1535235b",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "UPDATE gold.dbo.patientobservations SET deceasedDateTime = NULL WHERE first_name = 'Dion244' and last_name = 'Bergnaum523'"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "212a6af9-a578-4a16-8a78-6cd6a0ff50df",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "The `UPDATE` statement should've run at about 2x faster, largely due to Deletion Vectors being enabled. \n",
                "\n",
                "📌 **Challenge:** Use `DESCRIBE HISTORY` on the table and inpsect the `operationMetrics` columns for the two `UPDATE` operations to see the difference with Deletion Vectors enabled."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f9f6659-7efb-4abb-9a7d-4c9439f0d666",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bb3a4049-44f1-4e30-8132-7b9df2d7df6c",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "<details>\n",
                "  <summary><strong>🔑 Answer:</strong>  Click to expand the answer if you need help with this task</summary>\n",
                "\n",
                "~~~sql\n",
                "DESCRIBE HISTORY gold.dbo.patientobservations\n",
                "~~~\n",
                "  \n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ad88e44c-cde3-4316-a539-c87ea094db66",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "> ℹ️ _You can query Delta table history by creating a temp view and then querying it just like any table. Run the below to get a cleaner display comparing key write operation metrics._"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cef35dae-1ac5-4b2c-8469-d8c2a0cf7deb",
            "metadata": {
                "collapsed": false,
                "jupyter": {
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "spark.sql(\"DESCRIBE HISTORY gold.dbo.patientobservations\").createOrReplaceTempView(\"po_history\")\n",
                "history_df = spark.sql(\"\"\"\n",
                "    SELECT \n",
                "        version, \n",
                "        operationMetrics, \n",
                "        operationMetrics.numUpdatedRows, \n",
                "        operationMetrics.numCopiedRows, \n",
                "        operationMetrics.numDeletionVectorsAdded, \n",
                "        operationMetrics.rewriteTimeMs \n",
                "    FROM po_history \n",
                "    WHERE operation = 'UPDATE'\n",
                "\"\"\")\n",
                "display(history_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "38dc79a3-644c-40fc-8f8b-d9a15f7afdfb",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## **4.4 - Getting the Most Out of Table Clustering** | 🕑 5 min."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5c4f83f2-0eb5-4937-9d64-d52e5a8743f2",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## 📦 Table Clustering in Practice\n",
                "\n",
                "> Clustering controls **how data is physically laid out**, which affects **read efficiency**, **file skipping**, and **query performance**.\n",
                "\n",
                "---\n",
                "\n",
                "### 🧱 Partitioning (Traditional)\n",
                "\n",
                "✅ Best for **very large datasets** with **clear access patterns**.\n",
                "\n",
                "- 📏 **Avoid partitioning under 1 TB** — small tables get worse performance due to file and metadata overhead.\n",
                "- 🎯 **Pick low-cardinality columns** (e.g., `year`, `region`) — too many partitions = too many small files.\n",
                "- 🚫 Avoid partitioning by high-cardinality fields (e.g., `user_id`, `uuid`, `timestamp`) unless absolutely needed.\n",
                "- Optimized Write should generally be enabled when writing to partitioned tables to prevent too many small files.\n",
                "\n",
                "> 🧠 Good partitioning reduces scan ranges — bad partitioning just bloats your file count and adds additional write and read overhead.\n",
                "\n",
                "---\n",
                "\n",
                "### 🧮 Z-Order Clustering\n",
                "\n",
                "✅ Designed for **multi-dimensional skipping**, especially for **interactive queries**.\n",
                "\n",
                "- ⚠️ **Impractical for data production** — Z-Ordering is **not incremental**, and **not supported during write**.\n",
                "- 📦 Requires **periodic post-processing (OPTIMIZE ZORDER BY)**, which is **expensive** and often incompatible with streaming or near-real-time workflows.\n",
                "- 🧪 Use only if you're managing data for **heavy ad-hoc analytics workloads** and can afford the maintenance.\n",
                "\n",
                "> 💬 *\"Z-Order helps readers — but is very expensive for writers to maintain.\"*\n",
                "\n",
                "---\n",
                "\n",
                "### 🌊 Liquid Clustering\n",
                "\n",
                "🚧 **Not yet recommended** in Microsoft Fabric (as of OSS Delta 3.2).\n",
                "\n",
                "- ⚙️ OSS Delta lacks complete **write-time optimizations** (e.g., optimal clustering-aware writes, file grouping).\n",
                "- 🔍 No **read-time skipping optimizations** yet, so clustered data doesn't give file skipping benefits.\n",
                "- 🔁 It makes writer 20-25% slower and doesn't improve reads.\n",
                "\n",
                "> 📌 Until both **write-side layout** and **reader skipping logic** are improved, liquid clustering gives limited benefit.\n",
                "\n",
                "---\n",
                "\n",
                "### ✨ Summary Guidelines\n",
                "\n",
                "| Feature             | Use It When...                           | Avoid When...                                 |\n",
                "|---------------------|------------------------------------------|-----------------------------------------------|\n",
                "| **Partitioning**    | Table is >1TB, access is filterable      | Table is small or queries are random access   |\n",
                "| **Z-Order**         | You control refresh cadence, filter by multiple columns | You have streaming/real-time pipelines |\n",
                "| **Liquid Clustering** | (Not recommended yet in Fabric)         | (Not mature enough for benefit)               |\n",
                "\n",
                "---\n",
                "\n",
                "## 🔚 Rule of Thumb\n",
                "\n",
                "> **Don’t cluster by default — cluster with purpose.**\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "60c5a24d-d5af-4dff-91ec-5f1e702080eb",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## **4.5 - Right-Sizing Spark for the Job** | 🕑 5 min."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e6181e74-62c8-4f58-961a-e6415bc14cfd",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### ✅ **4.5.1 - General Guidance: Right-Sizing Spark Compute**\n",
                "\n",
                "<br>\n",
                "\n",
                "The key is to **match node size and count** to your workload’s shape and business goals — not just go big or small by default.\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "\n",
                "<br>\n",
                "\n",
                "#### 💡 **Best Practices**\n",
                "\n",
                "<br>\n",
                "\n",
                "- 🧠 **Consider workload shape and SLAs**:  \n",
                "  Are you optimizing for **cost**, **speed**, or a balance of both? If you need to hit tight SLAs, right-sizing means **spending smarter**, not just spending more.\n",
                "\n",
                "<br>\n",
                "\n",
                "- ⚖️ **Bigger clusters ≠ more expensive — if they finish proportionally faster**:  \n",
                "  A cluster that is 4× bigger and finishes your job 4× faster will cost about the **same** — but delivers results sooner and frees up compute for other jobs.\n",
                "\n",
                "<br>\n",
                "\n",
                "- 🚀 **Larger nodes typically reduce shuffle & GC overhead**:  \n",
                "  Fewer, larger executors often result in less shuffling, more efficient memory usage, and lower GC times.\n",
                "\n",
                "<br>\n",
                "\n",
                "- 🎛️ **Autoscale and Dynamic Allocation = flexibility & efficiency**  \n",
                "  These features shine when **your jobs vary in shape** or **you don't know exactly what the workload will look like**. They let Spark request resources based on actual needs — starting as single node, growing when busy, shrinking when idle. Autoscale tends to be somewhat _conservative_ in deciding when to scale in comparison to other platforms.\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "\n",
                "<br>\n",
                "\n",
                "#### 🔧 **Your Knobs**\n",
                "\n",
                "| Knob | Options |\n",
                "|------|---------|\n",
                "| **Node Size** | Small (4 cores) → Medium (8 cores) → Large (16 cores) → XL (32 cores) → XXL (64 cores) |\n",
                "| **# of Nodes** | 1 to N |\n",
                "| **Autoscale Range** | 1 to N nodes |\n",
                "| **Dynamic Allocation Range** | 1 to N nodes|\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "\n",
                "<br>\n",
                "\n",
                "#### 💡 **When to Use What**\n",
                "\n",
                "| Scenario | Guidance |\n",
                "|----------|----------|\n",
                "| **Transform-heavy jobs with shuffles & joins** | Use **larger nodes** (16–64 cores), fewer total nodes. Avoid tiny fragments. |\n",
                "| **Bursty or unpredictable jobs** | Use **Autoscale + Dynamic Allocate** to let the cluster grow/shrink as needed. Works well when jobs vary in size. |\n",
                "| **Many small parallel jobs (e.g., streaming or batch microjobs)** | Use **small/medium nodes**, more nodes. Set minimum nodes to avoid cold start lag. |\n",
                "| **Small serial processed jobs or development work** | Use **small or medium nodes** in single node mode (driver and executor shares 1 VM). |\n",
                "| **Large jobs with known partitioning** | Pre-size the cluster manually: pick the node size and count based on data volume and shuffle stages. Disable DA if it isn't scaling up aggressively enough to meet your performance needs. |\n",
                "| **ML or distributed training** | Use **many medium/large nodes** to maximize parallelism and distribute compute evenly. |\n",
                "\n",
                "<br>\n",
                "\n",
                "> ⚠️ **When using High-Concurrency Mode** the upper range of DA should be less than the maximum Autoscale range to prevent a single job from allocating all nodes in a long running operation.\n",
                "> <br>\n",
                "> <br>\n",
                ">_Outside of High-Concurrency Mode, Autoscale and Dynamic Allocation should be set to the same range._\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "\n",
                "<br>\n",
                "\n",
                "#### 🚩 **Red Flags That You're Mis-Sized**\n",
                "\n",
                "- **Long GC times** → Node too small or too many small executors  \n",
                "- **Shuffle spill to disk** → Not enough memory, or too many partitions per core  \n",
                "- **Slow start / long scheduling delay** → DA starting too low or nodes scaling up too late  \n",
                "- **Low CPU usage across nodes** → Too large, underutilized cluster or poorly parallelized job\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "\n",
                "<br>\n",
                "\n",
                "#### ⚖️ **Key Trade-Offs**\n",
                "\n",
                "- **Autoscale + DA = flexibility**, but may cause **warm-up lag** or inconsistent performance  \n",
                "- **Static sizing = predictability**, but risks **wasting resources** on smaller jobs  \n",
                "- **Larger nodes = better shuffle handling**, **fewer JVMs**, but less granular parallelism  \n",
                "- **Smaller nodes = fine-grained parallelism**, but more driver overhead per executor  \n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "\n",
                "<br>\n",
                "\n",
                "#### ⚠️ **Things to Avoid!**\n",
                "\n",
                "##### 🐢 Underpowered or Poorly Sized Compute\n",
                "\n",
                "- **Small clusters** may save money up front—but if they cause excessive **shuffling, spilling, or GC**, they end up costing more in time and dollars.\n",
                "- **Too many small executors** can be worse than **fewer large ones**, especially for shuffles and joins.\n",
                "- ✅ *Fix*: Right-size your compute for the workload. Use **metrics like shuffle spill, memory usage, and GC time** to guide tuning.\n",
                "\n",
                "<br>\n",
                "\n",
                "##### 🔄 Too Much Caching / Wrong Things Cached\n",
                "\n",
                "- Caching large DataFrames unnecessarily eats up executor memory and causes GC issues.\n",
                "- Worse, caching things that change between actions results in stale or invalid data.\n",
                "- ✅ *Fix*: Only cache when reuse is **frequent and beneficial**. Unpersist when done.\n",
                "\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "\n",
                "<br>\n",
                "\n",
                "#### 🔑 **Key Takeaway**\n",
                "\n",
                "Start with the **shape of your workload** and your **expected data processing SLAs**, not the shape of your cluster.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "32ce4977-125b-4d2a-91fd-63194a2dff39",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## **4.6 - Writing Smarter Queries** | 🕑 10 min.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e65adeb6-dae9-4488-8527-d29666bb3ad3",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 🎲 **4.6.1. Avoid Relying on Schema Inference in Production**\n",
                "\n",
                "- Schema inference slows down reads, **increases job execution time**, and can result in **unexpected schema drift**.\n",
                "- Particularly painful in high-scale scenarios.\n",
                "- ✅ *Fix*: Define schemas explicitly in production jobs, especially for semi-structured data like JSON and any structured files without a schema header (CSV, Excel, etc.).\n",
                "\n",
                "👉 _Let's explore how this impacts execution times. First, lets start with allowing Spark to automatically infer the schema._"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d7063ef-0ff5-474a-a568-2bb62510c0af",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import from_json,col\n",
                "from pyspark.sql.types import *\n",
                "\n",
                "observations_path = f\"abfss://47938747-73b4-4f78-99dc-4ff2afa78142@onelake.dfs.fabric.microsoft.com/443d992d-01f1-4caa-9345-088e81dd81df/Files/observationsrawfull/\"\n",
                "# Load JSON data\n",
                "observations_raw_df = spark.read.json(observations_path)\n",
                "\n",
                "display(observations_raw_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c5480511-e5c1-4a59-bc49-2e52179e31c6",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "We can get the underlying schema of any DataFrame via calling `.schema` on the DataFrame. \n",
                "\n",
                "📌 **Challenge:** Use the following cell to return the schema and then create a variable named `schema` with the schema that was returned."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "06a93d0f-30fb-4296-8f56-f869f5f11be5",
            "metadata": {
                "jupyter": {
                    "source_hidden": false
                },
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-24T22:18:08.8612007Z",
                            "execution_start_time": "2025-03-24T22:18:08.5987865Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "c183db82-63f2-45bb-9bbc-4e012e1d6ac6",
                            "queued_time": "2025-03-24T22:18:08.5976351Z",
                            "session_id": "b0d3c55e-45ca-4b9f-a54f-3db85457abd0",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 12,
                            "statement_ids": [
                                12
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, b0d3c55e-45ca-4b9f-a54f-3db85457abd0, 12, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "ce7781a6-61ee-4fe0-a3c5-2b33d05bde21",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "<details>\n",
                "  <summary><strong>🔑 Answer:</strong>  Click to expand the answer if you need help with this task</summary>\n",
                "\n",
                "~~~python\n",
                "# Print the schema\n",
                "print(observations_raw_df.schema)\n",
                "\n",
                "# Create a variable named schema\n",
                "schema = observations_raw_df.schema\n",
                "# OR\n",
                "schema = StructType([StructField('category', ArrayType(StructType([StructField('coding', ArrayType(StructType([StructField('code', StringType(), True), StructField('display', StringType(), True), StructField('system', StringType(), True)]), True), True)]), True), True), StructField('code', StructType([StructField('coding', ArrayType(StructType([StructField('code', StringType(), True), StructField('display', StringType(), True), StructField('system', StringType(), True)]), True), True), StructField('text', StringType(), True)]), True), StructField('component', ArrayType(StructType([StructField('code', StructType([StructField('coding', ArrayType(StructType([StructField('code', StringType(), True), StructField('display', StringType(), True), StructField('system', StringType(), True)]), True), True), StructField('text', StringType(), True)]), True), StructField('valueCodeableConcept', StructType([StructField('coding', ArrayType(StructType([StructField('code', StringType(), True), StructField('display', StringType(), True), StructField('system', StringType(), True)]), True), True), StructField('text', StringType(), True)]), True), StructField('valueQuantity', StructType([StructField('code', StringType(), True), StructField('system', StringType(), True), StructField('unit', StringType(), True), StructField('value', DoubleType(), True)]), True), StructField('valueString', StringType(), True)]), True), True), StructField('effectiveDateTime', StringType(), True), StructField('encounter', StructType([StructField('reference', StringType(), True)]), True), StructField('id', StringType(), True), StructField('issued', StringType(), True), StructField('meta', StructType([StructField('profile', ArrayType(StringType(), True), True)]), True), StructField('resourceType', StringType(), True), StructField('status', StringType(), True), StructField('subject', StructType([StructField('reference', StringType(), True)]), True), StructField('valueCodeableConcept', StructType([StructField('coding', ArrayType(StructType([StructField('code', StringType(), True), StructField('display', StringType(), True), StructField('system', StringType(), True)]), True), True), StructField('text', StringType(), True)]), True), StructField('valueQuantity', StructType([StructField('code', StringType(), True), StructField('system', StringType(), True), StructField('unit', StringType(), True), StructField('value', DoubleType(), True)]), True)])\n",
                "~~~\n",
                "  \n",
                "</details>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ccf747d3-951f-48e0-89fa-d49967c62496",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "Now run the code below to read the JSON files into a DataFrame, but with the statically defined schema to see the performance improvement. You should see much better performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "19fc73d3-3e0d-41bf-b553-a19347cf37c3",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import from_json,col\n",
                "observations_path = f\"abfss://47938747-73b4-4f78-99dc-4ff2afa78142@onelake.dfs.fabric.microsoft.com/443d992d-01f1-4caa-9345-088e81dd81df/Files/observationsrawfull/\"\n",
                "# Load JSON data\n",
                "observations_raw_df = spark.read.json(observations_path, schema)\n",
                "\n",
                "display(observations_raw_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "77bba843-ecc8-4d87-ae6f-803a8a6ae50e",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### ❓ **4.6.2 - Avoid Overusing Repartition or Coalesce Without Understanding**\n",
                "\n",
                "- Over-partitioning increases task overhead and shuffle volume. Under-partitioning can lead to skew and stragglers.\n",
                "- `repartition()` is rarely needed but can be extremely helpful when used intentionally and in the right scenarios.\n",
                "- ✅ *Fix*: Base partitioning decisions on **data size** and **number of output files**. Use the **Spark UI** to see partition/task counts. If you don't know what you are doing, don't repartition.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "11c6241d-7bd3-4f98-8d89-501096b80df1",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 🔄 **4.6.3 - Avoid Too Much Caching / Wrong Things Cached**\n",
                "\n",
                "- Caching large DataFrames unnecessarily eats up executor memory and causes GC issues.\n",
                "- Worse, caching things that change between actions results in stale or invalid data.\n",
                "- ✅ *Fix*: Only cache when reuse is **frequent and beneficial**. Unpersist when done.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d7abed1e-67be-46b7-a2ed-da63e7a0bd30",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 🧊 **4.6.4 - Avoid Using Expensive UDFs in Hot Paths**\n",
                "\n",
                "- Python, Scala, or Java UDFs are black boxes to the Catalyst optimizer.\n",
                "- They **disable query optimizations**, and often run **single-threaded** per executor task.\n",
                "- ✅ *Fix*: Rewrite logic using Spark SQL functions or **pandas_udf** (if applicable). Avoid UDFs in `WHERE`, `JOIN`, and `GROUP BY` clauses.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "11a55ea1-da7f-4933-a6c3-8f2f63a4693c",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 📥 **4.6.5 - Avoid Data Skew**\n",
                "\n",
                "- When a small number of keys have a disproportionate number of rows, tasks processing those keys become **stragglers**, slowing down the job.\n",
                "- ✅ *Fix*: Detect skew via Spark UI or metrics. Use **salting**, **adaptive skew join**, or data bucketing for heavily skewed keys.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b8bcfa05-355c-46a3-a3c1-bf878d3a510f",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## **4.7 - Keeping Tables Healthy** | 🕑 5 min."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "99a959ed-f9a4-4cb1-b303-4da3b237f96b",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### **📉 4.7.1 - Avoiding Small File Problems**\n",
                "- Delta Lake performance relies heavily on **file pruning** and **partition pruning**.\n",
                "- If your table has thousands of tiny files (or large unpartitioned ones), query planning and scanning become inefficient.\n",
                "- ✅ *Fix*: Use **Auto Compaction**, combined with **Optimized Write** where appropriate.\n",
                "\n",
                "<br>\n",
                "\n",
                "![Auto Compaction](https://milescole.dev/assets/img/posts/Compaction/auto-compaction.excalidraw.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ac2106d3-efac-4530-819a-c8b2ca14e953",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "### 🔁 **4.7.2 - Preventing Excessive Storage Costs**\n",
                "- Delta tables **accumulate metadata and file over time**. Without regular maintenance, the number of files will continue to accumulate. While running `OPTIMIZE` will prevent performance issues due to accumulating many files, `VACUUM` will help prevent unnessesary storage costs.\n",
                "- ✅ *Fix*: Set up recurring jobs to:\n",
                "  - Clean up obsolete data (`VACUUM`) -> Prevents excessive storage costs\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb3c46ed-76ab-4e58-a559-c19c59655e92",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### **4.7.2.1 - Run Vacuum to Clean Up Old Files**\n",
                "Delta has a safety check to prevent a possible concurrent long running write operations from have data deleted pre-commit. If you are certain that there are no operations being performed on this table that take longer than the retention interval you plan to specify, you can turn off this safety check by setting the Spark configuration property `spark.databricks.delta.retentionDurationCheck.enabled` to `false`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "48d90838-0d5a-4cec-8f05-fafe92faca2d",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-24T21:39:31.8392866Z",
                            "execution_start_time": "2025-03-24T21:39:31.5640475Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "0ea631c8-966f-4597-9894-26e937672ed4",
                            "queued_time": "2025-03-24T21:39:31.5628417Z",
                            "session_id": "765ae0fd-357a-4234-bf0a-0c317eeca3f7",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 29,
                            "statement_ids": [
                                29
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, 765ae0fd-357a-4234-bf0a-0c317eeca3f7, 29, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# allow low retention vacuum\n",
                "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cce0674d-b585-43f0-af34-b62de10447c1",
            "metadata": {
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "_Before_ running `VACUUM`, let's get a count of parquet files in the table's data directory:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a19d1846-99d9-4c24-a0c5-c0611e0d1f7a",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "files = notebookutils.fs.ls(f\"abfss://{notebookutils.runtime.context['currentWorkspaceName']}@{spark.conf.get('fs.defaultFS').split('@')[1]}gold.Lakehouse/Tables/dbo/patientobservations/\")\n",
                "all_files = [f.path for f in files if f.path.endswith('.parquet')]\n",
                "print(len(all_files))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "926507ea-7b4d-4710-9ad2-54eac0171292",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "Running `VACUUM` by default will delete inactive files from Delta versions older than 168 hours (7 days). You can customize the number of hours that are retained via adding `RETAIN N HOURS`. \n",
                "\n",
                "📌 **Challenge:** Modify the `VACUUM` statement below to **retain 0 hours** of history."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ef491bb9-7c31-4467-9678-34f0f385dd8d",
            "metadata": {
                "collapsed": false,
                "microsoft": {
                    "language": "sparksql",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "%%sql\n",
                "VACUUM gold.dbo.patientobservations"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "be5f7463-388b-4719-acf0-41f97ee6bf68",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "<details>\n",
                "  <summary><strong>🔑 Answer:</strong>  Click to expand the answer if you need help with this task</summary>\n",
                "\n",
                "~~~sql\n",
                "VACUUM gold.dbo.patientobservations RETAIN 0 HOURS\n",
                "~~~\n",
                "  \n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "66580882-bc63-4f1c-88ba-a05eb5211c1d",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "#### **4.7.2.2 - Verify Only Active Files Remain**\n",
                "Now that VACUUM has been run, let's verify that files pertaining to older table versions has been cleaned"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9715345a-9f78-4ed4-84f0-567018f66de6",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "files = notebookutils.fs.ls(f\"abfss://{notebookutils.runtime.context['currentWorkspaceName']}@{spark.conf.get('fs.defaultFS').split('@')[1]}gold.Lakehouse/Tables/dbo/patientobservations/\")\n",
                "all_files = [f.path for f in files if f.path.endswith('.parquet')]\n",
                "len(all_files)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "00282b63-c282-41cf-a2d6-361ec054c61e",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "To verify that these are files in the active Delta version, we can use `df.inputFiles()` to return the files used to return a query against the table."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e86046e8-5e3b-4ba4-a987-a5db1c6786b8",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [],
            "source": [
                "active_files = spark.sql(f\"SELECT * FROM gold.dbo.patientobservations\").inputFiles()\n",
                "active_files"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d117b7af-00e7-4b2f-b069-25ad7e8f6088",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "Now we'll convert the two lists to sets so that we can easily subtract the two to veryify that all files from old versions has been vacuumed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fe22061a-c58b-4bfd-b6c8-97456290e906",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2025-03-24T21:42:46.7673064Z",
                            "execution_start_time": "2025-03-24T21:42:46.4815817Z",
                            "livy_statement_state": "available",
                            "normalized_state": "finished",
                            "parent_msg_id": "0955ea0b-c05c-4e56-ae9a-3278ad3f6f9e",
                            "queued_time": "2025-03-24T21:42:46.4805074Z",
                            "session_id": "765ae0fd-357a-4234-bf0a-0c317eeca3f7",
                            "session_start_time": null,
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 41,
                            "statement_ids": [
                                41
                            ]
                        },
                        "text/plain": [
                            "StatementMeta(, 765ae0fd-357a-4234-bf0a-0c317eeca3f7, 41, Finished, Available, Finished)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "all_files = [f.path.split('/')[-1] for f in files if f.path.endswith('.parquet')]\n",
                "active_files = [f.split('/')[-1] for f in active_files]\n",
                "result = list(set(all_files) - set(active_files))\n",
                "assert len(result) == 0"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "54d74b46-f467-4d05-b526-406a6cdf1912",
            "metadata": {
                "microsoft": {
                    "language": "python",
                    "language_group": "synapse_pyspark"
                },
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## **BONUS: Cheat Sheet - Top Reasons for Sub-Optimal Performance**\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### ❌ **1. Not Using the Native Execution Engine**\n",
                "\n",
                "- If you aren't using the Native Execution, and thus operating on the traditional Spark JVM-based execution engine, you’re **missing out on huge performance gains**—we’re talking 2x–5x improvements in some cases.\n",
                "- ✅ *Fix*: Validate engine usage via the Spark UI or logs. Ensure compatible functions and workloads are used to **unlock native execution**.\n",
                "- 👉 *Deep dive in Section 4.2*\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### ⚠️ **2. Using Features Without a Use Case**\n",
                "\n",
                "- Features like **Deletion Vectors**, **Change Data Feed**, **Optimized Writes**, and **V-Order** are powerful—but they **add overhead**.\n",
                "- When enabled on high-ingest or read-intensive tables **without a clear need**, they increase metadata size, write cost, and memory usage.\n",
                "- ✅ *Fix*: Evaluate features **per table and per workload zone** based on workload requirements.\n",
                "- 👉 *Best practices are covered in Section 4.3*\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### 🌀 **3. Mixing Execution Contexts**\n",
                "\n",
                "- Combining **Spark, Pandas, Polars, DuckDB**, etc. in a single pipeline adds **serialization overhead**, breaks optimizations, and introduces **data duplication in memory**.\n",
                "- While flexible, it reduces the optimizer’s ability to plan holistically.\n",
                "- ✅ *Fix*: Stick to one processing engine per stage of the pipeline. Use cross-context conversions **sparingly and intentionally**.\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### 🐢 **4. Underpowered or Poorly Sized Compute**\n",
                "\n",
                "- **Small clusters** may save money up front—but if they cause excessive **shuffling, spilling, or GC**, they end up costing more in time and dollars.\n",
                "- **Too many small executors** can be worse than **fewer large ones**, especially for shuffles and joins.\n",
                "- ✅ *Fix*: Right-size your compute for the workload. Use **metrics like shuffle spill, memory usage, and GC time** to guide tuning.\n",
                "- 👉 *Discussed in Section 4.5*\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### 🎲 **5. Relying on Schema Inference in Production**\n",
                "\n",
                "- Schema inference slows down reads, increases job startup time, and can result in **unexpected schema drift**.\n",
                "- Particularly painful in high-scale scenarios.\n",
                "- ✅ *Fix*: Define schemas explicitly in production jobs, especially for semi-structured data like JSON and any structured files without a schema header (CSV, Excel, etc.).\n",
                "- 👉 *Discussed in Section 4.6*\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### 📉 **6. Poor File Layout and Small File Problem**\n",
                "\n",
                "- Delta Lake performance relies heavily on **file pruning** and **partition pruning**.\n",
                "- If your table has thousands of tiny files (or large unpartitioned ones), query planning and scanning become inefficient.\n",
                "- ✅ *Fix*: Use **Auto Compaction**, combined with **Optimized Write** where appropriate.\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### 🔁 **7. No Table Maintenance Strategy**\n",
                "- Delta tables **accumulate metadata and file churn**. Without regular maintenance, performance **gradually decays**.\n",
                "- ✅ *Fix*: Set up recurring jobs to:\n",
                "  - Compact files (`OPTIMIZE`) -> Maintains performance\n",
                "  - Clean up obsolete data (`VACUUM`) -> Prevents excessive storage costs\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### ❓ **8. Overusing Repartition or Coalesce Without Understanding**\n",
                "\n",
                "- Over-partitioning increases task overhead and shuffle volume. Under-partitioning can lead to skew and stragglers.\n",
                "- `repartition()` is rarely needed but can be extremely helpful when used intentionally and in the right scenarios.\n",
                "- ✅ *Fix*: Base partitioning decisions on **data size** and **number of output files**. Use the **Spark UI** to see partition/task counts. If you don't know what you are doing, don't repartition.\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### 📊 **9. Poor Partitioning Strategy**\n",
                "\n",
                "- Partitioning is generally only beneficial for tables > 1Tb in compressed size. Partitioning below 1Tb generally hurts read performance and causes unnessesary write overhead.\n",
                "- Partitioning on high cardinality columns will quickly results in **small-file problems**.\n",
                "- ✅ *Fix*: Choose partition columns based on **query patterns**, **cardinality**, and **evolution over time**. Evaluate regularly.\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### 🔄 **10. Too Much Caching / Wrong Things Cached**\n",
                "\n",
                "- Caching large DataFrames unnecessarily eats up executor memory and causes GC issues.\n",
                "- Worse, caching things that change between actions results in stale or invalid data.\n",
                "- ✅ *Fix*: Only cache when reuse is **frequent and beneficial**. Unpersist when done.\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### 🧊 **11. Using Expensive UDFs in Hot Paths**\n",
                "\n",
                "- Python, Scala, or Java UDFs are black boxes to the Catalyst optimizer.\n",
                "- They **disable query optimizations**, and often run **single-threaded** per executor task.\n",
                "- ✅ *Fix*: Rewrite logic using Spark SQL functions or **pandas_udf** (if applicable). Avoid UDFs in `WHERE`, `JOIN`, and `GROUP BY` clauses.\n",
                "\n",
                "<br>\n",
                "\n",
                "---\n",
                "<br>\n",
                "\n",
                "###### 📥 **12. Lack of Data Skew Handling**\n",
                "\n",
                "- When a small number of keys have a disproportionate number of rows, tasks processing those keys become **stragglers**, slowing down the job.\n",
                "- ✅ *Fix*: Detect skew via Spark UI or metrics. Use **salting**, **adaptive skew join**, or data bucketing for heavily skewed keys.\n",
                "\n",
                "<br>\n"
            ]
        }
    ],
    "metadata": {
        "a365ComputeOptions": null,
        "dependencies": {
            "environment": {},
            "lakehouse": {}
        },
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Synapse PySpark",
            "language": "Python",
            "name": "synapse_pyspark"
        },
        "language_info": {
            "name": "python"
        },
        "microsoft": {
            "language": "python",
            "language_group": "synapse_pyspark",
            "ms_spell_check": {
                "ms_spell_check_language": "en"
            }
        },
        "nteract": {
            "version": "nteract-front-end@1.0.0"
        },
        "sessionKeepAliveTimeout": 0,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.synapse.nbs.session.timeout": "1800000"
                }
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
